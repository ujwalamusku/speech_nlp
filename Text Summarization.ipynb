{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vyom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt') # one time execution\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"speech.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "df.columns = ['Timestamp', 'Speaker', 'Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 2 options – we can either summarize each article individually, or we can generate a single summary for all the articles. For our purpose, we will go ahead with the latter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to break the text into individual sentences. We will use the sent_tokenize( ) function of the nltk library to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentences = []\n",
    "for s in df['Text']:\n",
    "  sentences.append(sent_tokenize(s))\n",
    "\n",
    "sentences = [y for x in sentences for y in x] # flatten list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tim, how are we looking on your focus with the top 5 accounts this quarter?',\n",
       " 'So just to recap, we identified the top 5 opportunities as Boots, Dell, Tesco Mobile, Peugeot and Adidas.',\n",
       " 'Right now we are in a good place on all except Adidas.',\n",
       " 'I’ll come back to Adidas in a minute.',\n",
       " 'The headlines against the others are Boots are now spending over £1million with us on Create and we have some exciting opportunities to use DCO in a really customised execution based on both the products that are trending and and the current weather conditions.',\n",
       " 'That’s brilliant - i’m really excited about this DCO opportunity - that has the potential to be an amazing story to spread throughout MediaCom.',\n",
       " 'Tell me about it - I’m just keeping my fingers crossed this execution goes smoothly given all the problems we have had with Jivox in the UK.',\n",
       " 'So, carrying on through the other accounts.',\n",
       " 'Hey Bob, why dont you take us through Dell since you are closest to it?',\n",
       " 'sure thing.',\n",
       " 'For Dell we are taking the great work done in the US by our friends over the pond and trying to replicate the same analytics engagement here.',\n",
       " 'In short, they want to create a custom KPI framework that considers their business growth KPIs.',\n",
       " 'Our media spend is already strong on Dell, but this custom KPI project would be key to getting the business more sticky into Q1.',\n",
       " 'I am hitting a few problems here - the European lead is really hard to meet with and has cancelled on me three times.',\n",
       " 'Ok, is this Gavin Reeder?',\n",
       " 'Yes thats him.',\n",
       " 'Ok i know him really well.',\n",
       " 'He went to my friend’s wedding a couple years back and we shared a table.',\n",
       " 'Want me to get in touch?',\n",
       " 'Yeah, if you could that would be grand.',\n",
       " 'No worries, leave that with me - i’ll sort this.',\n",
       " 'Ok thanks Fran.',\n",
       " 'Shall we continue going through the others',\n",
       " 'Yeah go ahead',\n",
       " 'We dont have a load of time left so I am going to be a little brief - especially as i want to get Adidas on your radar before we finish this morning.',\n",
       " 'So Tesco Mobile and Peugeot are both in really solid positions.',\n",
       " 'We’re currently taking £300k this quarter on Tesco and £500k on Peugeot.',\n",
       " 'Both of these pieces of business are totally incremental on last year given we didnt see a penny in 2018.',\n",
       " 'We’re working with Raj to get Tesco Mobile into a stronger analytics position.',\n",
       " 'The strategic focus they have given us is to better understand user journeys and media effectiveness now that we are back live with our Motion beta in Europe.',\n",
       " 'This makes me a little nervous given we havent thoroughly tested Motion but Shah has told me it will be ok.',\n",
       " 'Ok i get why you are nervous here.',\n",
       " 'Can you please let Damien know that this is a top priority for us - we’ve got one shot at getting this right on Tesco so we cant blow it because of a product issue.',\n",
       " 'Okay good shout - let me speak to Damien.',\n",
       " 'I might need your support in him prioritising as i know he has a lot on his plate',\n",
       " 'Of course, can you slack both Freddie and Damien and let them know the context behind this test.',\n",
       " 'Put me on it too and ill mention it to Freddie myself in our catch-up.',\n",
       " 'Perfect, thanks.',\n",
       " 'No worries!',\n",
       " 'Hey guys just letting you know we’ve only got 5minutes left.',\n",
       " 'Ah shit, my bad - lets quickly get on to Adidas and i’ll come back to Peugeot next week.',\n",
       " 'You guys know we’ve been trying to crack Adidas for some time now.',\n",
       " 'Despite my relationship with Luke we keep getting the cold shoulder given their insistence to do everything themselves.',\n",
       " 'The challenge i am getting is that they are all in on DV360 and dont want to work with us on a managed service.',\n",
       " 'They appear to have their own traders running the campaigns and dont work with other players such as Quantcast or Captify.',\n",
       " 'Did you get a sense from Luke what their top challenges or strategic focus is for 2020?',\n",
       " 'Yeah let me just check my notes….two seconds….',\n",
       " 'Ok...so he said the top priority is developing a custom measurement framework.',\n",
       " 'He mentioned something called ADH - but god knows what that is.',\n",
       " 'ADH is Ads data hub.',\n",
       " 'Tom Richards was telling me all about this over a beer last week.',\n",
       " 'Ads data hub is where Google have created a privacy first environment to process event level data to do a range of things - such as building out custom measurement models.',\n",
       " 'Tom mentioned a PM who was working on ADH in Bangalore.',\n",
       " 'Bob, do you know who that is?',\n",
       " 'Yeah that’s Abhishek Chakraborty - we met when i was there last month and he mentioned he was really keen to get some POCs under our belt in key markets.',\n",
       " 'Could be we brief him and see what he thinks?',\n",
       " 'Great shout.',\n",
       " 'Tim, can you sort this out please',\n",
       " 'Yes of course - shall i let Tom know too?',\n",
       " 'Yeah i reckon so - it sounded like he wanted to stay pretty close to it.',\n",
       " 'Ok - i’ll sort that out with chit chat now.',\n",
       " 'Thanks.',\n",
       " 'Brilliant.',\n",
       " 'We’re going to get kicked out in a minute so i just wanted to say thanks for making this an efficient meet.',\n",
       " 'Ok to summarise the actions',\n",
       " 'I am going to speak with Gavin on Dell',\n",
       " 'Tim to speak with Damien about the priority location test on Tesco',\n",
       " 'Tim to give Tom & Abhishek the context for an ADH POC on Adidas',\n",
       " 'And I will send Giles the chit chat content summary after this meeting']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing a sample for the sentences\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download GloVe Word Embeddings** <br>\n",
    "GloVe word embeddings are vector representation of words. These word embeddings will be used to create vectors for our sentences. We could have also used the Bag-of-Words or TF-IDF approaches to create features for our sentences, but these methods ignore the order of the words (and the number of features is usually pretty large)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the pre-trained Wikipedia 2014 + Gigaword 5 GloVe vectors available in the link below. Heads up – the size of these word embeddings is 822 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ont time install for wget package\n",
    "# !pip install wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using wget, we'll download the embeddings file on local and call it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract word vectors\n",
    "word_embeddings = {}\n",
    "\n",
    "f = open('glove6B100d.txt', encoding='utf-8')\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have word vectors for 400,000 different terms stored in the dictionary – ‘word_embeddings’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Preprocessing**<br>\n",
    "It is always a good practice to make your textual data noise-free as much as possible. So, let’s do some basic text cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuations, numbers and special characters\n",
    "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "\n",
    "# make alphabets lowercase\n",
    "clean_sentences = [s.lower() for s in clean_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of the stopwords (commonly used words of a language – is, am, the, of, in, etc.) present in the sentences. If you have not downloaded nltk-stopwords, then execute the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vyom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove stopwords\n",
    "def remove_stopwords(sen):\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords from the sentences\n",
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vector Representation of Sentences**<br>\n",
    "We will use clean_sentences to create vectors for sentences in our data with the help of the GloVe word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "for i in clean_sentences:\n",
    "  if len(i) != 0:\n",
    "    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "  else:\n",
    "    v = np.zeros((100,))\n",
    "  sentence_vectors.append(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similarity Matrix Preparation**<br>\n",
    "The next step is to find similarities between the sentences, and we will use the cosine similarity approach for this challenge. Let’s create an empty similarity matrix for this task and populate it with cosine similarities of the sentences.\n",
    "<br>\n",
    "Let’s first define a zero matrix of dimensions (n * n).  We will initialize this matrix with cosine similarity scores of the sentences. Here, n is the number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity matrix\n",
    "sim_mat = np.zeros([len(sentences), len(sentences)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Cosine Similarity to compute the similarity between a pair of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# initialize the matrix with cosine similarity scores\n",
    "for i in range(len(sentences)):\n",
    "  for j in range(len(sentences)):\n",
    "    if i != j:\n",
    "      sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applying PageRank Algorithm**<br>\n",
    "Before proceeding further, let’s convert the similarity matrix sim_mat into a graph. The nodes of this graph will represent the sentences and the edges will represent the similarity scores between the sentences. On this graph, we will apply the PageRank algorithm to arrive at the sentence rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary Extraction**<br>\n",
    "Finally, it’s time to extract the top N sentences based on their rankings for summary generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.DataFrame(sentences).reset_index()\n",
    "df_temp.columns = ['Timestamp', 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Tim, how are we looking on your focus with the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>So just to recap, we identified the top 5 oppo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Right now we are in a good place on all except...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>I’ll come back to Adidas in a minute.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>The headlines against the others are Boots are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>Ok to summarise the actions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>I am going to speak with Gavin on Dell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>Tim to speak with Damien about the priority lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>Tim to give Tom &amp; Abhishek the context for an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "      <td>And I will send Giles the chit chat content su...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Timestamp                                               Text\n",
       "0           0  Tim, how are we looking on your focus with the...\n",
       "1           1  So just to recap, we identified the top 5 oppo...\n",
       "2           2  Right now we are in a good place on all except...\n",
       "3           3              I’ll come back to Adidas in a minute.\n",
       "4           4  The headlines against the others are Boots are...\n",
       "..        ...                                                ...\n",
       "64         64                        Ok to summarise the actions\n",
       "65         65             I am going to speak with Gavin on Dell\n",
       "66         66  Tim to speak with Damien about the priority lo...\n",
       "67         67  Tim to give Tom & Abhishek the context for an ...\n",
       "68         68  And I will send Giles the chit chat content su...\n",
       "\n",
       "[69 rows x 2 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We dont have a load of time left so I am going to be a little brief - especially as i want to get Adidas on your radar before we finish this morning.\n",
      "Can you please let Damien know that this is a top priority for us - we’ve got one shot at getting this right on Tesco so we cant blow it because of a product issue.\n",
      "We’re going to get kicked out in a minute so i just wanted to say thanks for making this an efficient meet.\n",
      "The challenge i am getting is that they are all in on DV360 and dont want to work with us on a managed service.\n",
      "Ok i know him really well.\n"
     ]
    }
   ],
   "source": [
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "\n",
    "# Extract top 10 sentences as the summary\n",
    "for i in range(5):\n",
    "  print(ranked_sentences[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_df = pd.DataFrame(ranked_sentences)\n",
    "ranked_df.columns = ['Score', 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.merge(df_temp, ranked_df, on = 'Text').sort_values(by = 'Score', ascending =  False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tell me about it - I’m just keeping my fingers crossed this execution goes smoothly given all the problems we have had with Jivox in the UK.',\n",
       " 'Hey Bob, why dont you take us through Dell since you are closest to it?',\n",
       " 'For Dell we are taking the great work done in the US by our friends over the pond and trying to replicate the same analytics engagement here.',\n",
       " 'Our media spend is already strong on Dell, but this custom KPI project would be key to getting the business more sticky into Q1.',\n",
       " 'I am hitting a few problems here - the European lead is really hard to meet with and has cancelled on me three times.',\n",
       " 'Ok i know him really well.',\n",
       " 'Want me to get in touch?',\n",
       " 'We dont have a load of time left so I am going to be a little brief - especially as i want to get Adidas on your radar before we finish this morning.',\n",
       " 'Both of these pieces of business are totally incremental on last year given we didnt see a penny in 2018.',\n",
       " 'The strategic focus they have given us is to better understand user journeys and media effectiveness now that we are back live with our Motion beta in Europe.',\n",
       " 'Can you please let Damien know that this is a top priority for us - we’ve got one shot at getting this right on Tesco so we cant blow it because of a product issue.',\n",
       " 'I might need your support in him prioritising as i know he has a lot on his plate',\n",
       " 'Of course, can you slack both Freddie and Damien and let them know the context behind this test.',\n",
       " 'Ah shit, my bad - lets quickly get on to Adidas and i’ll come back to Peugeot next week.',\n",
       " 'You guys know we’ve been trying to crack Adidas for some time now.',\n",
       " 'Despite my relationship with Luke we keep getting the cold shoulder given their insistence to do everything themselves.',\n",
       " 'The challenge i am getting is that they are all in on DV360 and dont want to work with us on a managed service.',\n",
       " 'Yeah that’s Abhishek Chakraborty - we met when i was there last month and he mentioned he was really keen to get some POCs under our belt in key markets.',\n",
       " 'Could be we brief him and see what he thinks?',\n",
       " 'We’re going to get kicked out in a minute so i just wanted to say thanks for making this an efficient meet.']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(final.head(20).sort_values('Timestamp')['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
